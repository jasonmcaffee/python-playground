{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# configure the client's secret key\n",
    "with open('secret-key.txt', 'r') as file:\n",
    "    secret_key = file.read().strip()\n",
    "openai.api_key = secret_key\n",
    "\n",
    "max_tokens = 4092"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "import time\n",
    "def prompt(question):\n",
    "    start_time_seconds = time.time()\n",
    "    print(f\"asking chatgpt: {question}\")\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\", #\"text-davinci-003\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions accurately, without making up facts.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "    )\n",
    "    # print(response)\n",
    "    choices = response.choices\n",
    "    choice = choices[0]\n",
    "    message = choice['message']\n",
    "    answer = message['content']\n",
    "    print(answer)\n",
    "    print(f\"answer received in {time.time() - start_time_seconds} seconds.\")\n",
    "\n",
    "def stream_prompt(question):\n",
    "    start_time_seconds = time.time()\n",
    "    print(f\"asking chatgpt: {question}\")\n",
    "    stream = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\", #\"text-davinci-003\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions accurately, without making up facts.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        stream=True,\n",
    "    )\n",
    "    for output in stream:\n",
    "        choices = output['choices']\n",
    "        choice = choices[0]\n",
    "        delta = choice['delta']\n",
    "        if 'content' in delta:\n",
    "            print(delta['content'], end='')\n",
    "    print()\n",
    "    print(f\"answer received in {time.time() - start_time_seconds} seconds.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asking chatgpt: \n",
      "What is a camel?\n",
      "\n",
      "A camel is a large mammal with a humped back, long legs, and a long neck. It is native to the deserts of Africa and Asia and is well adapted to living in hot and arid conditions. Camels are known for their ability to store water in their humps, enabling them to survive in environments with limited water sources. They are often used as pack animals and are also raised for their milk, meat, and hair.\n",
      "answer received in 10.847567319869995 seconds.\n"
     ]
    }
   ],
   "source": [
    "stream_prompt(\"\"\"\n",
    "What is a camel?\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb\n",
    "import json\n",
    "\n",
    "def get_function_calls_details_from_llm_response(response):\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.get('tool_calls')\n",
    "    results = None\n",
    "    # Step 2: check if the model wanted to call a function\n",
    "    if tool_calls:\n",
    "        results = []\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            arguments = tool_call.function.arguments\n",
    "            result = {\n",
    "                \"function_name\": function_name,\n",
    "                \"arguments\": arguments,\n",
    "                # data needed for subsequent calls to chatgpt\n",
    "                \"metadata\": {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": None, # todo: populate this once the function has been executed.\n",
    "                },\n",
    "            }\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "def function_call_prompt(question):\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_user_details\",\n",
    "                \"description\": \"Retrieves information for a user based on the combination of their first and last names.  It returns information about the user's age, location, and profession.\",\n",
    "                \"parameters\":{\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"first_name\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"First name of the user to get details of.\"\n",
    "                        },\n",
    "                        \"last_name\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Last name of the user to get details of.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"first_name\", \"last_name\"]\n",
    "                }\n",
    "            },\n",
    "\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    start_time_seconds = time.time()\n",
    "    print(f\"asking chatgpt: {question}\")\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions accurately, without making up facts.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        tools=tools\n",
    "    )\n",
    "    print(response)\n",
    "    functions = get_function_calls_details_from_llm_response(response)\n",
    "    if functions is not None:\n",
    "        for function in functions:\n",
    "            if function['function_name'] == 'get_user_details':\n",
    "                content = get_user_details(function['arguments'])\n",
    "                function['metadata']['content'] = content\n",
    "\n",
    "    print(f\"answer received in {time.time() - start_time_seconds} seconds.\")\n",
    "\n",
    "# function that will be called, based on the LLM's response, which will indicate that it wants the function to be called, along with the arguments\n",
    "# arguments are a json string.  e.g. '{\"first_name\": \"Jason\", \"last_name: \"McAffee\"}'\n",
    "def get_user_details(args):\n",
    "    arguments = json.loads(args)\n",
    "    print(f\"get_user_details function called with arguments first_name: {arguments['first_name']}, last_name: {arguments['last_name']}\")\n",
    "    response = {\n",
    "        \"age\": 44,\n",
    "        \"location\": {\n",
    "            \"state\": \"Utah\"\n",
    "        },\n",
    "        \"profession\": \"Software Engineer\"\n",
    "    }\n",
    "    return response"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asking chatgpt: \n",
      "Who is user Jason McAffee?\n",
      "\n",
      "{\n",
      "  \"id\": \"chatcmpl-8OdsAEvqPHUWMlLYShPRlc90SizCy\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1700883834,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"id\": \"call_HnsHtNPVKC6rVYXYEG2duSs3\",\n",
      "            \"type\": \"function\",\n",
      "            \"function\": {\n",
      "              \"name\": \"get_user_details\",\n",
      "              \"arguments\": \"{\\n  \\\"first_name\\\": \\\"Jason\\\",\\n  \\\"last_name\\\": \\\"McAffee\\\"\\n}\"\n",
      "            }\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"finish_reason\": \"tool_calls\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 123,\n",
      "    \"completion_tokens\": 27,\n",
      "    \"total_tokens\": 150\n",
      "  }\n",
      "}\n",
      "get_user_details function called with arguments first_name: Jason, last_name: McAffee\n",
      "answer received in 3.895408868789673 seconds.\n"
     ]
    }
   ],
   "source": [
    "function_call_prompt(\"\"\"\n",
    "Who is user Jason McAffee?\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asking chatgpt: \n",
      "What is a camel?\n",
      "\n",
      "{\n",
      "  \"id\": \"chatcmpl-8Odq2EJGwwj7M8zc3q79VAMonyXWV\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1700883702,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"A camel is a large mammal with a humped back, long legs, and a long neck. It is commonly found in the desert regions of Africa and Asia. Camels are well-adapted to desert life and have the ability to store water in their humps. They are used by humans for transportation and as pack animals in arid regions.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 120,\n",
      "    \"completion_tokens\": 73,\n",
      "    \"total_tokens\": 193\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "tool_calls",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/openai/openai_object.py:59\u001B[0m, in \u001B[0;36mOpenAIObject.__getattr__\u001B[0;34m(self, k)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[0;31mKeyError\u001B[0m: 'tool_calls'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[38], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mfunction_call_prompt\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;43mWhat is a camel?\u001B[39;49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[36], line 65\u001B[0m, in \u001B[0;36mfunction_call_prompt\u001B[0;34m(question)\u001B[0m\n\u001B[1;32m     56\u001B[0m response \u001B[38;5;241m=\u001B[39m openai\u001B[38;5;241m.\u001B[39mChatCompletion\u001B[38;5;241m.\u001B[39mcreate(\n\u001B[1;32m     57\u001B[0m     model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt-3.5-turbo\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     58\u001B[0m     messages\u001B[38;5;241m=\u001B[39m[\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     62\u001B[0m     tools\u001B[38;5;241m=\u001B[39mtools\n\u001B[1;32m     63\u001B[0m )\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28mprint\u001B[39m(response)\n\u001B[0;32m---> 65\u001B[0m functions \u001B[38;5;241m=\u001B[39m \u001B[43mget_function_calls_details_from_llm_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m functions \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m function \u001B[38;5;129;01min\u001B[39;00m functions:\n",
      "Cell \u001B[0;32mIn[36], line 6\u001B[0m, in \u001B[0;36mget_function_calls_details_from_llm_response\u001B[0;34m(response)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_function_calls_details_from_llm_response\u001B[39m(response):\n\u001B[1;32m      5\u001B[0m     response_message \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mchoices[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mmessage\n\u001B[0;32m----> 6\u001B[0m     tool_calls \u001B[38;5;241m=\u001B[39m \u001B[43mresponse_message\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtool_calls\u001B[49m\n\u001B[1;32m      7\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;66;03m# Step 2: check if the model wanted to call a function\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/openai/openai_object.py:61\u001B[0m, in \u001B[0;36mOpenAIObject.__getattr__\u001B[0;34m(self, k)\u001B[0m\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m[k]\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m---> 61\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;241m*\u001B[39merr\u001B[38;5;241m.\u001B[39margs)\n",
      "\u001B[0;31mAttributeError\u001B[0m: tool_calls"
     ]
    }
   ],
   "source": [
    "function_call_prompt(\"\"\"\n",
    "What is a camel?\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}