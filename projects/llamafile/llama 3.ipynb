{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x /Users/jmcaffee/llm_models/Meta-Llama-3-8B-Instruct.Q5_K_M.llamafile\n",
    "!chmod +x /Users/jmcaffee/llm_models/Meta-Llama-3-70B-Instruct.Q4_0.llamafile\n",
    "!chmod +x /Users/jmcaffee/llm_models/Llama-3-8B-Instruct-32k-v0.1.Q4_K_M.gguf\n",
    "!sh -c /Users/jmcaffee/llm_models/Meta-Llama-3-8B-Instruct.Q5_K_M.llamafile -ngl 9999 --server\n",
    "!. /Users/jmcaffee/llm_models/Meta-Llama-3-8B-Instruct.Q5_K_M.llamafile -ngl 9999 --server --nobrowser\n",
    "# !sh -c /Users/jmcaffee/llm_models/Meta-Llama-3-70B-Instruct.Q4_0.llamafile -ngl 9999"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T14:22:04.155185Z",
     "start_time": "2024-05-06T14:10:13.030303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# windows\n",
    "%cd c:\\shared-drive\\llm_models\n",
    "!dir\n",
    "!.\\llamafile-0.8.1.exe -m \"Meta-Llama-3-70B-Instruct.Q4_0.llamafile.exe\" --server -ngl 9999\n",
    "# !.\\Meta-Llama-3-70B-Instruct.Q4_0.llamafile.exe --server --nobrowser -ngl 9999"
   ],
   "id": "8ec1253ce21b9c6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\shared-drive\\llm_models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is B622-4C9D\n",
      "\n",
      " Directory of c:\\shared-drive\\llm_models\n",
      "\n",
      "05/06/2024  07:08 AM    <DIR>          .\n",
      "05/06/2024  07:08 AM    <DIR>          ..\n",
      "05/04/2024  03:46 PM     4,920,733,952 Llama-3-8B-Instruct-32k-v0.1.Q4_K_M.gguf\n",
      "05/06/2024  07:08 AM        28,104,177 llamafile-0.8.1.exe\n",
      "05/05/2024  04:59 PM    34,268,006,848 Meta-Llama-3-70B-Instruct.Q3_K_M.gguf\n",
      "05/05/2024  07:24 PM    39,994,659,426 Meta-Llama-3-70B-Instruct.Q4_0.llamafile.exe\n",
      "05/04/2024  04:19 PM    49,950,329,280 Meta-Llama-3-70B-Instruct.Q5_K_M.gguf\n",
      "               5 File(s) 129,161,833,683 bytes\n",
      "               2 Dir(s)  1,711,701,446,656 bytes free\n",
      "^C\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bb6b7e3694dd57fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://github.com/meta-llama/llama-recipes\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "{{ system_prompt }}\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "{{ user_message_1 }}\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{{ model_answer_1 }}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{{ user_message_2 }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4535e656cb58022b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://127.0.0.1:8080/v1\", # \"THIS MUST BE 127.0.0.1, not localhost\n",
    "    api_key = \"sk-no-key-required\"\n",
    ")\n",
    "\n",
    "def stream_response(question):\n",
    "    \n",
    "    system_message = \"You are ChatGPT, an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests.\"\n",
    "    # system_prompt = f\"\"\"\n",
    "    #     <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    #     {system_message}\n",
    "    # \"\"\"\n",
    "    # user_prompt = f\"\"\"\n",
    "    #     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    #     {question}\n",
    "    # \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"llama-3\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        stream=True,  # Whether to stream the response or return the full response at once\n",
    "        #max_tokens=2000,  # The maximum number of tokens to generate in the completion\n",
    "        temperature=0.5,  # The temperature of the model, controlling the randomness of the output\n",
    "        top_p=1.0,  # The nucleus sampling parameter, controlling the diversity of the output\n",
    "        n=1,  # The number of completions to generate\n",
    "        stop=None,  # A list of stop sequences to stop the completion at\n",
    "        presence_penalty=0.0,  # The penalty for generating tokens that appear in the prompt\n",
    "        frequency_penalty=0.0,  # The penalty for generating tokens that appear frequently in the prompt\n",
    "        logit_bias={},  # A dictionary of token IDs and associated biases to apply to the logits\n",
    "    )\n",
    "    \n",
    "    # print(completion.choices[0].message)\n",
    "    for chunk in completion:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T10:34:29.254359Z",
     "start_time": "2024-05-02T10:34:29.243332Z"
    }
   },
   "id": "e6fc39c54c56c17f",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a fascinating topic! The discovery and exploration of America is a rich and complex chapter in world history. Here are some notable events:\n",
      "\n",
      "1. **Viking Exploration (1000 AD):** The Vikings, led by Leif Erikson, established a settlement in present-day Canada, about 500 years before Christopher Columbus's famous voyage.\n",
      "2. **Christopher Columbus's First Voyage (1492):** Columbus, sponsored by the Spanish monarchs, set sail across the Atlantic, landing in the Caribbean, specifically in present-day Haiti and the Dominican Republic.\n",
      "3. **The Age of Exploration (1492-1600s):** Many European explorers, such as Vasco da Gama, Ferdinand Magellan, and Juan Sebastián Elcano, ventured into the New World, discovering new lands, establishing trade routes, and claiming territories for their respective countries.\n",
      "4. **The Spanish Conquest (1519-1521):** Hernán Cortés led a Spanish expedition to Mexico, conquering the Aztec Empire and claiming the region for Spain.\n",
      "5. **The English Settlements (1607-1624):** The English established the Jamestown settlement in present-day Virginia, followed by the Plymouth Colony in present-day Massachusetts, marking the beginning of English colonization in North America.\n",
      "6. **The French Exploration (1534-1608):** French explorers, such as Jacques Cartier, mapped the St. Lawrence River and established settlements in present-day Canada, including Quebec City.\n",
      "7. **The Pilgrims' Journey (1620):** The Pilgrims sailed from England to North America, landing at Plymouth Rock in present-day Massachusetts, establishing the Plymouth Colony.\n",
      "8. **The California Missions (1769-1823):** Spanish Franciscan missionaries, led by Father Junípero Serra, established a series of missions in present-day California, marking the beginning of Spanish colonization in the region.\n",
      "\n",
      "These events represent a small sample of the many significant discoveries and explorations that shaped the history of America. Would you like me to elaborate on any of these events or explore other aspects of American history?None"
     ]
    }
   ],
   "source": [
    "stream_response(f\"\"\"\n",
    "What are some notable events in the discovery and exploration of america?\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T10:34:41.223107Z",
     "start_time": "2024-05-02T10:34:31.004166Z"
    }
   },
   "id": "46b08885176b321",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the table, here are the key takeaways:\n",
      "\n",
      "1. **item_type**: This field indicates the type of item being requested, such as a bank account or credit card.\n",
      "2. **institution_id**: This field is the unique identifier for the institution, such as a bank or credit card company.\n",
      "3. **client_id**: This field is the unique identifier for the client, which is typically a financial institution or a Plaid-developed application.\n",
      "4. **access_token**: This field is the access token used to authenticate the request.\n",
      "5. **refresh_token**: This field is the refresh token used to obtain a new access token when the current one expires.\n",
      "6. **expires_at**: This field is the timestamp indicating when the access token will expire.\n",
      "7. **item.update_type**: This field indicates whether an item requires user interaction to be updated, which can be the case for items with two-factor authentication.\n",
      "8. **request_id**: This field is a unique identifier for the request, which can be used for troubleshooting.\n",
      "\n",
      "Note that some fields may be null or empty depending on the specific use case and institution. Additionally, some fields may have specific requirements or restrictions for certain institutions or use cases. It's essential |assistant |assistant |assistant |assistant |assistant |assistant |assistant |assistant |assistant |assistant |assistant |assistant |assistant |assistant |assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant|assistant"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../parse_openapi/plaid-openai-parse-results/plaid_accounts_get.html\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[1;32m      2\u001B[0m     large_text \u001B[38;5;241m=\u001B[39m file\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m----> 4\u001B[0m \u001B[43mstream_response\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;43mSummarize the following text:\u001B[39;49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mlarge_text\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m)\u001B[49m    \n",
      "Cell \u001B[0;32mIn[18], line 37\u001B[0m, in \u001B[0;36mstream_response\u001B[0;34m(question)\u001B[0m\n\u001B[1;32m     19\u001B[0m completion \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mchat\u001B[38;5;241m.\u001B[39mcompletions\u001B[38;5;241m.\u001B[39mcreate(\n\u001B[1;32m     20\u001B[0m     model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllama-3\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     21\u001B[0m     messages\u001B[38;5;241m=\u001B[39m[\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     33\u001B[0m     logit_bias\u001B[38;5;241m=\u001B[39m{},  \u001B[38;5;66;03m# A dictionary of token IDs and associated biases to apply to the logits\u001B[39;00m\n\u001B[1;32m     34\u001B[0m )\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# print(completion.choices[0].message)\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m completion:\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;28mprint\u001B[39m(chunk\u001B[38;5;241m.\u001B[39mchoices[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdelta\u001B[38;5;241m.\u001B[39mcontent, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/openai/_streaming.py:46\u001B[0m, in \u001B[0;36mStream.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[_T]:\n\u001B[0;32m---> 46\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator:\n\u001B[1;32m     47\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m item\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/openai/_streaming.py:58\u001B[0m, in \u001B[0;36mStream.__stream__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     55\u001B[0m process_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39m_process_response_data\n\u001B[1;32m     56\u001B[0m iterator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter_events()\n\u001B[0;32m---> 58\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sse \u001B[38;5;129;01min\u001B[39;00m iterator:\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m sse\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[DONE]\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     60\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/openai/_streaming.py:50\u001B[0m, in \u001B[0;36mStream._iter_events\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_iter_events\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[ServerSentEvent]:\n\u001B[0;32m---> 50\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoder\u001B[38;5;241m.\u001B[39miter_bytes(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39miter_bytes())\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/openai/_streaming.py:280\u001B[0m, in \u001B[0;36mSSEDecoder.iter_bytes\u001B[0;34m(self, iterator)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21miter_bytes\u001B[39m(\u001B[38;5;28mself\u001B[39m, iterator: Iterator[\u001B[38;5;28mbytes\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[ServerSentEvent]:\n\u001B[1;32m    279\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Given an iterator that yields raw binary data, iterate over it & yield every event encountered\"\"\"\u001B[39;00m\n\u001B[0;32m--> 280\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter_chunks(iterator):\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;66;03m# Split before decoding so splitlines() only uses \\r and \\n\u001B[39;00m\n\u001B[1;32m    282\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m raw_line \u001B[38;5;129;01min\u001B[39;00m chunk\u001B[38;5;241m.\u001B[39msplitlines():\n\u001B[1;32m    283\u001B[0m             line \u001B[38;5;241m=\u001B[39m raw_line\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/openai/_streaming.py:291\u001B[0m, in \u001B[0;36mSSEDecoder._iter_chunks\u001B[0;34m(self, iterator)\u001B[0m\n\u001B[1;32m    289\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Given an iterator that yields raw binary data, iterate over it and yield individual SSE chunks\"\"\"\u001B[39;00m\n\u001B[1;32m    290\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 291\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m iterator:\n\u001B[1;32m    292\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m chunk\u001B[38;5;241m.\u001B[39msplitlines(keepends\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m    293\u001B[0m         data \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m line\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/httpx/_models.py:829\u001B[0m, in \u001B[0;36mResponse.iter_bytes\u001B[0;34m(self, chunk_size)\u001B[0m\n\u001B[1;32m    827\u001B[0m chunker \u001B[38;5;241m=\u001B[39m ByteChunker(chunk_size\u001B[38;5;241m=\u001B[39mchunk_size)\n\u001B[1;32m    828\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request):\n\u001B[0;32m--> 829\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m raw_bytes \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miter_raw():\n\u001B[1;32m    830\u001B[0m         decoded \u001B[38;5;241m=\u001B[39m decoder\u001B[38;5;241m.\u001B[39mdecode(raw_bytes)\n\u001B[1;32m    831\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m chunker\u001B[38;5;241m.\u001B[39mdecode(decoded):\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/httpx/_models.py:887\u001B[0m, in \u001B[0;36mResponse.iter_raw\u001B[0;34m(self, chunk_size)\u001B[0m\n\u001B[1;32m    884\u001B[0m chunker \u001B[38;5;241m=\u001B[39m ByteChunker(chunk_size\u001B[38;5;241m=\u001B[39mchunk_size)\n\u001B[1;32m    886\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request):\n\u001B[0;32m--> 887\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m raw_stream_bytes \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream:\n\u001B[1;32m    888\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_bytes_downloaded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(raw_stream_bytes)\n\u001B[1;32m    889\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m chunker\u001B[38;5;241m.\u001B[39mdecode(raw_stream_bytes):\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/httpx/_client.py:124\u001B[0m, in \u001B[0;36mBoundSyncStream.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m typing\u001B[38;5;241m.\u001B[39mIterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[0;32m--> 124\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stream:\n\u001B[1;32m    125\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m chunk\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/httpx/_transports/default.py:111\u001B[0m, in \u001B[0;36mResponseStream.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m typing\u001B[38;5;241m.\u001B[39mIterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[1;32m    110\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[0;32m--> 111\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m part \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_httpcore_stream:\n\u001B[1;32m    112\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m part\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:367\u001B[0m, in \u001B[0;36mPoolByteStream.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    365\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m    366\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n\u001B[0;32m--> 367\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:363\u001B[0m, in \u001B[0;36mPoolByteStream.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    361\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[1;32m    362\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 363\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m part \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stream:\n\u001B[1;32m    364\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m part\n\u001B[1;32m    365\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:336\u001B[0m, in \u001B[0;36mHTTP11ConnectionByteStream.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    334\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ShieldCancellation():\n\u001B[1;32m    335\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n\u001B[0;32m--> 336\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:328\u001B[0m, in \u001B[0;36mHTTP11ConnectionByteStream.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    326\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    327\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreceive_response_body\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request, kwargs):\n\u001B[0;32m--> 328\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connection\u001B[38;5;241m.\u001B[39m_receive_response_body(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    329\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m chunk\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m    331\u001B[0m     \u001B[38;5;66;03m# If we get an exception while streaming the response,\u001B[39;00m\n\u001B[1;32m    332\u001B[0m     \u001B[38;5;66;03m# we want to close the response (and possibly the connection)\u001B[39;00m\n\u001B[1;32m    333\u001B[0m     \u001B[38;5;66;03m# before raising that exception.\u001B[39;00m\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:197\u001B[0m, in \u001B[0;36mHTTP11Connection._receive_response_body\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    194\u001B[0m timeout \u001B[38;5;241m=\u001B[39m timeouts\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mread\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 197\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_receive_event\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h11\u001B[38;5;241m.\u001B[39mData):\n\u001B[1;32m    199\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mbytes\u001B[39m(event\u001B[38;5;241m.\u001B[39mdata)\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:211\u001B[0m, in \u001B[0;36mHTTP11Connection._receive_event\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    208\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_h11_state\u001B[38;5;241m.\u001B[39mnext_event()\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m event \u001B[38;5;129;01mis\u001B[39;00m h11\u001B[38;5;241m.\u001B[39mNEED_DATA:\n\u001B[0;32m--> 211\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_network_stream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    212\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mREAD_NUM_BYTES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[1;32m    213\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001B[39;00m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    221\u001B[0m     \u001B[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001B[39;00m\n\u001B[1;32m    222\u001B[0m     \u001B[38;5;66;03m# it as a ConnectError.\u001B[39;00m\n\u001B[1;32m    223\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;241m==\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_h11_state\u001B[38;5;241m.\u001B[39mtheir_state \u001B[38;5;241m==\u001B[39m h11\u001B[38;5;241m.\u001B[39mSEND_RESPONSE:\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/httpcore/_backends/sync.py:126\u001B[0m, in \u001B[0;36mSyncStream.read\u001B[0;34m(self, max_bytes, timeout)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39msettimeout(timeout)\n\u001B[0;32m--> 126\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_bytes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "with open('../parse_openapi/plaid-openai-parse-results/plaid_accounts_get.html', 'r') as file:\n",
    "    large_text = file.read()\n",
    "    \n",
    "stream_response(f\"\"\"\n",
    "Summarize the following text:\n",
    "{large_text}\n",
    "\"\"\")    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T10:41:35.968201Z",
     "start_time": "2024-05-02T10:41:17.342932Z"
    }
   },
   "id": "bf9152ad7329424f",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fa9fc7fb88b5cf12",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
