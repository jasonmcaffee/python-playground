{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This is a poc notebook for how to have @AI listen for slack messages, send them to the sagemaker endpoint, and stream the response back.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a6032803ae04a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "class LineIterator:\n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord('\\n'):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if 'PayloadPart' not in chunk:\n",
    "                print('Unknown event type:' + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk['PayloadPart']['Bytes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# stream https://aws.amazon.com/blogs/machine-learning/elevating-the-generative-ai-experience-introducing-streaming-support-in-amazon-sagemaker-hosting/\n",
    "# run !gimme-aws-creds, then aws_switch sofi-bedrock-bdp-production\n",
    "import boto3\n",
    "import io\n",
    "import json \n",
    "import sagemaker\n",
    "\n",
    "# session = sagemaker.Session()\n",
    "\n",
    "# session = boto3.Session(profile_name='sofi-bedrock-bdp-production')\n",
    "session = boto3.Session()\n",
    "smr = session.client('sagemaker-runtime', 'us-west-2')\n",
    "#AssumeBedrockPOC\n",
    "\n",
    "def sagemaker_prompt(question, on_text_received):\n",
    "    print(f\"sagemaker question: {question}\")\n",
    "    system_message = \"You are a helpful assistant that does not use superfluous pleasantries. Avoiding ending your reply with questions.  If a question does not make sense, call out why it doesn't make sense, and don't attempt to answer. If you don't know the answer to a question, do not make up an answer.\"\n",
    "    prompt=f'''[INST] <<SYS>>\n",
    "        {system_message}\n",
    "        <</SYS>>\n",
    "        {question} [/INST]'''\n",
    "    \n",
    "    # hyperparameters for llm\n",
    "    payload = {\n",
    "      \"inputs\": prompt,\n",
    "      \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.7,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_k\": 10,\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "        \"stop\": [\"<|endoftext|>\"]\n",
    "      },\n",
    "      \"stream\": True  \n",
    "    }\n",
    "    stop_token = \"<|endoftext|>\"\n",
    "    # llm.deserializer=StreamDeserializer()\n",
    "    endpoint_name = \"Llama-2-13B-Chat-fp16-v7-2023-12-21-17-51-07-902\"\n",
    "    resp = smr.invoke_endpoint_with_response_stream(EndpointName=endpoint_name, Body=json.dumps(payload), ContentType='application/json')\n",
    "    event_stream = resp['Body']\n",
    "    start_json = b'{'\n",
    "    for line in LineIterator(event_stream):\n",
    "        if line != b'' and start_json in line:\n",
    "            data = json.loads(line[line.find(start_json):].decode('utf-8'))\n",
    "            if data['token']['text'] != stop_token:\n",
    "                on_text_received(data['token']['text'])\n",
    "                # print(data['token']['text'],end='')\n",
    "\n",
    "def handle_text_received(text):\n",
    "    print(text, end='')\n",
    "\n",
    "sagemaker_prompt(\"What is 5 + 5?\", handle_text_received)       "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fb1a53c06fffd18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import threading\n",
    "nest_asyncio.apply()\n",
    "from slack_sdk.rtm_v2 import RTMClient\n",
    "\n",
    "# Load your Slack API token\n",
    "with open('slack-api-token.txt', 'r') as file:\n",
    "    slack_token = file.read().strip()\n",
    "\n",
    "# Initialize the RTMClient\n",
    "rtm = RTMClient(token=slack_token)\n",
    "\n",
    "# Define the event handler\n",
    "@rtm.on(\"message\")\n",
    "def handle(client: RTMClient, event: dict):\n",
    "    # print(f\"event: {event}\")\n",
    "    message_received = event.get('text', '')\n",
    "    # channel_id = event['channel']\n",
    "    # thread_ts = event['ts']\n",
    "    # user = event['user']  # User ID\n",
    "    ai_bot_user = \"<@U06B7U1478S>\"\n",
    "    \n",
    "    # Post a message to the channel\n",
    "    # Check if the message is addressed to @ai\n",
    "    if message_received.startswith(ai_bot_user):\n",
    "        # print('ai_bot_user')\n",
    "        \n",
    "        def handle_ai_bot_message(client: RTMClient, event: dict):\n",
    "            # print(f'handle ai bot message: {event}')\n",
    "            message_received = event.get('text', '')\n",
    "            channel_id = event['channel']\n",
    "            thread_ts = event['ts']\n",
    "            \n",
    "            prompt = message_received.replace(ai_bot_user, '')\n",
    "            cumulative_text = f\"{prompt}\\n\"\n",
    "            # print(cumulative_text)\n",
    "            # Initial message sent to the channel\n",
    "            initial_response = client.web_client.chat_postMessage(\n",
    "                channel=channel_id,\n",
    "                text=cumulative_text,\n",
    "                thread_ts=thread_ts\n",
    "            )\n",
    "    \n",
    "            # Retrieve timestamp of the initial response\n",
    "            initial_response_ts = initial_response['ts']\n",
    "            last_update_length = len(cumulative_text)\n",
    "            \n",
    "            def handle_text_received(text):\n",
    "                nonlocal cumulative_text, last_update_length\n",
    "                cumulative_text += text\n",
    "                # print(text, end='')\n",
    "                # Edit the existing message with the new text\n",
    "                if len(cumulative_text) - last_update_length >= 20 or \"</s>\" in cumulative_text:\n",
    "                    client.web_client.chat_update(\n",
    "                        channel=channel_id,\n",
    "                        ts=initial_response_ts,  # Timestamp of the message to update\n",
    "                        text=cumulative_text,\n",
    "                        thread_ts=thread_ts\n",
    "                    )\n",
    "                    last_update_length = len(cumulative_text)\n",
    "    \n",
    "            sagemaker_prompt(prompt, handle_text_received)\n",
    "\n",
    "        thread = threading.Thread(\n",
    "            target=handle_ai_bot_message,\n",
    "            args=(client, event)\n",
    "        )\n",
    "        thread.start()\n",
    "        # handle_ai_bot_message(client, event)\n",
    "    \n",
    "# Running the event loop\n",
    "# asyncio.ensure_future(rtm.start())\n",
    "# asyncio.get_event_loop().run_forever()\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "try:\n",
    "    asyncio.ensure_future(rtm.start())\n",
    "    loop.run_forever()\n",
    "except KeyboardInterrupt:\n",
    "    # Handle the interrupt gracefully\n",
    "    print(\"Interrupted by user, shutting down.\")\n",
    "finally:\n",
    "    # Perform any cleanup here if necessary\n",
    "    loop.stop()\n",
    "    # loop.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd85b548adc9d35"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
