{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages/slack/deprecation.py:14: UserWarning: slack package is deprecated. Please use slack_sdk.web/webhook/rtm package instead. For more info, go to https://slack.dev/python-slack-sdk/v3-migration/\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "# \n",
    "# import asyncio\n",
    "# from slack import RTMClient\n",
    "# \n",
    "# @RTMClient.run_on(event=\"message\")\n",
    "# def list_message(**payload):\n",
    "#     data = payload['data']\n",
    "#     print(data)  # Handle the incoming message data as per your requirements\n",
    "# \n",
    "# with open('slack-api-token.txt', 'r') as file:\n",
    "#     slack_token = file.read().strip()\n",
    "# \n",
    "# rtm_client = RTMClient(token=slack_token)\n",
    "# asyncio.ensure_future(rtm_client.start())\n",
    "# asyncio.get_event_loop().run_forever()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-15T02:52:53.341194Z"
    }
   },
   "id": "d8d15f75e148b9b4"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-15T03:01:52.290344Z",
     "start_time": "2023-12-15T03:01:49.827894Z"
    }
   },
   "outputs": [],
   "source": [
    "# from slack_sdk.rtm_v2 import RTMClient\n",
    "# with open('slack-api-token.txt', 'r') as file:\n",
    "#     slack_token = file.read().strip()\n",
    "# \n",
    "# rtm = RTMClient(token=slack_token)\n",
    "# \n",
    "# @rtm.on(\"message\")\n",
    "# def handle(client: RTMClient, event: dict):\n",
    "#     if 'Hello' in event['text']:\n",
    "#         channel_id = event['channel']\n",
    "#         thread_ts = event['ts']\n",
    "#         user = event['user'] # This is not username but user ID (the format is either U*** or W***)\n",
    "# \n",
    "#         client.web_client.chat_postMessage(\n",
    "#             channel=channel_id,\n",
    "#             text=f\"Hi <@{user}>!\",\n",
    "#             thread_ts=thread_ts\n",
    "#         )\n",
    "# \n",
    "# rtm.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Volumes/Macintosh HD/Users/jmcaffee/llm-models/llama-2-7b-chat.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q5_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:             blk.10.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:        blk.10.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:             blk.10.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.10.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:        blk.11.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:             blk.11.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:             blk.11.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:             blk.12.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:        blk.12.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:             blk.12.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:             blk.12.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.13.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:        blk.13.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:             blk.13.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:             blk.14.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:        blk.14.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:             blk.14.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:           blk.15.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:           blk.15.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:             blk.15.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:             blk.15.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:        blk.15.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:             blk.15.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:             blk.15.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:           blk.16.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:           blk.16.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:             blk.16.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:             blk.16.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:        blk.16.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.16.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:             blk.16.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:           blk.17.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.17.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.17.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:             blk.17.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:        blk.17.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.17.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:             blk.17.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:           blk.18.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:           blk.18.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.18.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.18.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:        blk.18.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:             blk.18.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.18.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.19.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:           blk.19.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.19.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.19.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:        blk.19.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:             blk.19.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.19.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.2.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:            blk.2.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:              blk.2.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.2.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:         blk.2.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:              blk.2.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:              blk.2.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.20.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.20.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.20.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.20.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.20.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.20.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.20.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.21.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.21.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.21.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.21.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.21.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.21.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.21.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.22.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.22.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.22.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.22.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.22.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.22.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.22.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.23.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.23.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.23.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.23.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.23.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.23.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.23.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:            blk.3.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:            blk.3.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:              blk.3.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:              blk.3.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:         blk.3.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:              blk.3.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:              blk.3.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:            blk.4.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:            blk.4.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:              blk.4.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:              blk.4.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:         blk.4.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:              blk.4.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:              blk.4.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:            blk.5.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:            blk.5.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:              blk.5.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:              blk.5.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:         blk.5.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:              blk.5.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:              blk.5.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:            blk.6.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:            blk.6.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:              blk.6.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:              blk.6.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:         blk.6.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:              blk.6.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:              blk.6.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:            blk.7.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:            blk.7.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:              blk.7.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:              blk.7.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:         blk.7.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:              blk.7.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:              blk.7.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:            blk.8.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:            blk.8.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:              blk.8.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:              blk.8.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:         blk.8.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:              blk.8.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:              blk.8.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:            blk.9.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:            blk.9.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:              blk.9.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:              blk.9.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:         blk.9.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:              blk.9.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:              blk.9.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:           blk.24.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.24.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:        blk.24.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:           blk.25.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.25.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:        blk.25.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.26.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:           blk.26.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.26.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:        blk.26.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.attn_v.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:           blk.27.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.27.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:        blk.27.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:           blk.28.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.28.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:        blk.28.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:           blk.29.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.29.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:        blk.29.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:           blk.30.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:           blk.30.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.30.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:        blk.30.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q5_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 4.45 GiB (5.68 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.12 MiB\n",
      "llm_load_tensors: mem required  = 4560.98 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 159.32 MiB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "max_tokens = 2048\n",
    "# llama_llm = Llama(model_path=\"/Volumes/4Tera_SSD_2022/llm_models/llama-2-7b-chat.Q5_K_M.gguf\", chat_format=\"llama-2\")\n",
    "llm = Llama(model_path=\"/Volumes/Macintosh HD/Users/jmcaffee/llm-models/llama-2-7b-chat.Q5_K_M.gguf\", chat_format=\"llama-2\", n_ctx=max_tokens)\n",
    "llm.verbose = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T03:16:31.049871Z",
     "start_time": "2023-12-15T03:16:30.253363Z"
    }
   },
   "id": "56ff3b219290883f"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import time\n",
    "def prompt2(question):\n",
    "    start_time_seconds = time.time()\n",
    "    output = llm.create_chat_completion(\n",
    "        max_tokens=max_tokens,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an assistant who perfectly responds to questions with as many details as possible, without making up facts.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    elapsed_time_seconds = time.time() - start_time_seconds\n",
    "    # print(output)\n",
    "    answer = output['choices'][0]['message']['content']\n",
    "    print(f'answer generated in {elapsed_time_seconds} seconds')\n",
    "    print(f\"Q: {question} \\nA:{answer}\")\n",
    "    return answer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T03:18:00.940512Z",
     "start_time": "2023-12-15T03:18:00.937999Z"
    }
   },
   "id": "a94cbedcdb1dc3d"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event: {'type': 'message', 'channel': 'D06ATFU9MAT', 'text': 'Hello', 'blocks': [{'type': 'rich_text', 'block_id': 'HnjxV', 'elements': [{'type': 'rich_text_section', 'elements': [{'type': 'text', 'text': 'Hello'}]}]}], 'user': 'U02EJ467WLX', 'client_msg_id': 'f5626b6d-8305-4041-84ac-05226e9c738f', 'team': 'TB14JBH0C', 'source_team': 'E01BQQJKZQS', 'user_team': 'E01BQQJKZQS', 'suppress_notification': False, 'event_ts': '1703183272.832849', 'ts': '1703183272.832849'}\n",
      "event: {'type': 'message', 'subtype': 'message_replied', 'message': {'client_msg_id': 'f5626b6d-8305-4041-84ac-05226e9c738f', 'type': 'message', 'text': 'Hello', 'user': 'U02EJ467WLX', 'ts': '1703183272.832849', 'blocks': [{'type': 'rich_text', 'block_id': 'HnjxV', 'elements': [{'type': 'rich_text_section', 'elements': [{'type': 'text', 'text': 'Hello'}]}]}], 'team': 'TB14JBH0C', 'thread_ts': '1703183272.832849', 'reply_count': 1, 'reply_users_count': 1, 'latest_reply': '1703183273.302039', 'reply_users': ['B06BLH6F9S5'], 'is_locked': False}, 'channel': 'D06ATFU9MAT', 'hidden': True, 'ts': '1703183272.832849', 'event_ts': '1703183273.000100'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 37\u001B[0m\n\u001B[1;32m     24\u001B[0m     client\u001B[38;5;241m.\u001B[39mweb_client\u001B[38;5;241m.\u001B[39mchat_postMessage(\n\u001B[1;32m     25\u001B[0m         channel\u001B[38;5;241m=\u001B[39mchannel_id,\n\u001B[1;32m     26\u001B[0m         text\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHaving llm answer the question: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmessage_received\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     27\u001B[0m         thread_ts\u001B[38;5;241m=\u001B[39mthread_ts\n\u001B[1;32m     28\u001B[0m     )\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;66;03m# answer = prompt2(message_received)\u001B[39;00m\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;66;03m# client.web_client.chat_postMessage(\u001B[39;00m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;66;03m#     channel=channel_id,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;66;03m# )\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Running the event loop\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m asyncio\u001B[38;5;241m.\u001B[39mensure_future(\u001B[43mrtm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     38\u001B[0m asyncio\u001B[38;5;241m.\u001B[39mget_event_loop()\u001B[38;5;241m.\u001B[39mrun_forever()\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/slack_sdk/rtm_v2/__init__.py:270\u001B[0m, in \u001B[0;36mRTMClient.start\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    268\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Establishes an RTM connection and blocks the current thread.\"\"\"\u001B[39;00m\n\u001B[1;32m    269\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconnect()\n\u001B[0;32m--> 270\u001B[0m \u001B[43mEvent\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:581\u001B[0m, in \u001B[0;36mEvent.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    579\u001B[0m signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flag\n\u001B[1;32m    580\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[0;32m--> 581\u001B[0m     signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    582\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:312\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[1;32m    311\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 312\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    313\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    314\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from slack_sdk.rtm_v2 import RTMClient\n",
    "\n",
    "# Load your Slack API token\n",
    "with open('slack-api-token.txt', 'r') as file:\n",
    "    slack_token = file.read().strip()\n",
    "\n",
    "# Initialize the RTMClient\n",
    "rtm = RTMClient(token=slack_token)\n",
    "\n",
    "# Define the event handler\n",
    "@rtm.on(\"message\")\n",
    "def handle(client: RTMClient, event: dict):\n",
    "    print(f\"event: {event}\")\n",
    "    message_received = event.get('text', '')\n",
    "    channel_id = event['channel']\n",
    "    thread_ts = event['ts']\n",
    "    user = event['user']  # User ID\n",
    "\n",
    "    \n",
    "    # Post a message to the channel\n",
    "    client.web_client.chat_postMessage(\n",
    "        channel=channel_id,\n",
    "        text=f\"Having llm answer the question: {message_received}\",\n",
    "        thread_ts=thread_ts\n",
    "    )\n",
    "    \n",
    "    answer = prompt2(message_received)\n",
    "    client.web_client.chat_postMessage(\n",
    "        channel=channel_id,\n",
    "        text=f\"Answer: {answer}\",\n",
    "        thread_ts=thread_ts\n",
    "    )\n",
    "# Running the event loop\n",
    "asyncio.ensure_future(rtm.start())\n",
    "asyncio.get_event_loop().run_forever()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T18:28:37.399901Z",
     "start_time": "2023-12-21T18:27:30.877397Z"
    }
   },
   "id": "cedba2ac4bedc656"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker\r\n",
      "  Downloading sagemaker-2.201.0-py2.py3-none-any.whl (1.4 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.4/1.4 MB\u001B[0m \u001B[31m2.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m0m\r\n",
      "\u001B[?25hRequirement already satisfied: psutil in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from sagemaker) (5.9.6)\r\n",
      "Collecting docker\r\n",
      "  Downloading docker-7.0.0-py3-none-any.whl (147 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m147.6/147.6 kB\u001B[0m \u001B[31m5.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: numpy<2.0,>=1.9.0 in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from sagemaker) (1.26.2)\r\n",
      "Requirement already satisfied: PyYAML~=6.0 in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from sagemaker) (6.0.1)\r\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from sagemaker) (4.25.1)\r\n",
      "Collecting pathos\r\n",
      "  Using cached pathos-0.3.1-py3-none-any.whl (82 kB)\r\n",
      "Collecting tblib==1.7.0\r\n",
      "  Using cached tblib-1.7.0-py2.py3-none-any.whl (12 kB)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from sagemaker) (23.2)\r\n",
      "Collecting schema\r\n",
      "  Using cached schema-0.7.5-py2.py3-none-any.whl (17 kB)\r\n",
      "Requirement already satisfied: urllib3<1.27 in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from sagemaker) (1.26.18)\r\n",
      "Collecting importlib-metadata<7.0,>=1.4.0\r\n",
      "  Using cached importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\r\n",
      "Collecting boto3<2.0,>=1.33.3\r\n",
      "  Downloading boto3-1.34.5-py3-none-any.whl (139 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m139.3/139.3 kB\u001B[0m \u001B[31m5.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting uvicorn==0.22.0\r\n",
      "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m58.3/58.3 kB\u001B[0m \u001B[31m2.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: requests in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from sagemaker) (2.31.0)\r\n",
      "Collecting cloudpickle==2.2.1\r\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\r\n",
      "Collecting google-pasta\r\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\r\n",
      "Collecting smdebug-rulesconfig==1.0.1\r\n",
      "  Using cached smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\r\n",
      "Requirement already satisfied: jsonschema in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from sagemaker) (4.20.0)\r\n",
      "Collecting fastapi==0.95.2\r\n",
      "  Downloading fastapi-0.95.2-py3-none-any.whl (56 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m57.0/57.0 kB\u001B[0m \u001B[31m1.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: platformdirs in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from sagemaker) (4.1.0)\r\n",
      "Requirement already satisfied: tqdm in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from sagemaker) (4.66.1)\r\n",
      "Collecting pandas\r\n",
      "  Downloading pandas-2.1.4-cp39-cp39-macosx_10_9_x86_64.whl (11.8 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.8/11.8 MB\u001B[0m \u001B[31m13.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: attrs<24,>=23.1.0 in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from sagemaker) (23.1.0)\r\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from fastapi==0.95.2->sagemaker) (0.27.0)\r\n",
      "Collecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2\r\n",
      "  Downloading pydantic-1.10.13-cp39-cp39-macosx_10_9_x86_64.whl (2.9 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.9/2.9 MB\u001B[0m \u001B[31m9.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m0:01\u001B[0mm\r\n",
      "\u001B[?25hRequirement already satisfied: click>=7.0 in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from uvicorn==0.22.0->sagemaker) (8.1.7)\r\n",
      "Requirement already satisfied: h11>=0.8 in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from uvicorn==0.22.0->sagemaker) (0.14.0)\r\n",
      "Collecting s3transfer<0.10.0,>=0.9.0\r\n",
      "  Downloading s3transfer-0.9.0-py3-none-any.whl (82 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m82.0/82.0 kB\u001B[0m \u001B[31m2.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting jmespath<2.0.0,>=0.7.1\r\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\r\n",
      "Collecting botocore<1.35.0,>=1.34.5\r\n",
      "  Downloading botocore-1.34.5-py3-none-any.whl (11.9 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.9/11.9 MB\u001B[0m \u001B[31m14.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: zipp>=0.5 in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.17.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from requests->sagemaker) (2023.11.17)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from requests->sagemaker) (3.6)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from requests->sagemaker) (3.3.2)\r\n",
      "Requirement already satisfied: six in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from google-pasta->sagemaker) (1.16.0)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from jsonschema->sagemaker) (0.13.2)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from jsonschema->sagemaker) (0.32.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from jsonschema->sagemaker) (2023.11.2)\r\n",
      "Collecting tzdata>=2022.1\r\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from pandas->sagemaker) (2.8.2)\r\n",
      "Collecting pytz>=2020.1\r\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\r\n",
      "Collecting pox>=0.3.3\r\n",
      "  Using cached pox-0.3.3-py3-none-any.whl (29 kB)\r\n",
      "Collecting dill>=0.3.7\r\n",
      "  Using cached dill-0.3.7-py3-none-any.whl (115 kB)\r\n",
      "Collecting multiprocess>=0.70.15\r\n",
      "  Using cached multiprocess-0.70.15-py39-none-any.whl (133 kB)\r\n",
      "Collecting ppft>=1.7.6.7\r\n",
      "  Using cached ppft-1.7.6.7-py3-none-any.whl (56 kB)\r\n",
      "Collecting contextlib2>=0.5.5\r\n",
      "  Using cached contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi==0.95.2->sagemaker) (4.9.0)\r\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker) (4.1.0)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker) (1.3.0)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/jmcaffee/git/python-playground-v2/venv/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker) (1.2.0)\r\n",
      "Installing collected packages: pytz, uvicorn, tzdata, tblib, smdebug-rulesconfig, pydantic, ppft, pox, jmespath, importlib-metadata, google-pasta, dill, contextlib2, cloudpickle, schema, pandas, multiprocess, docker, botocore, s3transfer, pathos, fastapi, boto3, sagemaker\r\n",
      "  Attempting uninstall: uvicorn\r\n",
      "    Found existing installation: uvicorn 0.24.0.post1\r\n",
      "    Uninstalling uvicorn-0.24.0.post1:\r\n",
      "      Successfully uninstalled uvicorn-0.24.0.post1\r\n",
      "  Attempting uninstall: pydantic\r\n",
      "    Found existing installation: pydantic 2.5.2\r\n",
      "    Uninstalling pydantic-2.5.2:\r\n",
      "      Successfully uninstalled pydantic-2.5.2\r\n",
      "  Attempting uninstall: importlib-metadata\r\n",
      "    Found existing installation: importlib-metadata 7.0.0\r\n",
      "    Uninstalling importlib-metadata-7.0.0:\r\n",
      "      Successfully uninstalled importlib-metadata-7.0.0\r\n",
      "  Attempting uninstall: fastapi\r\n",
      "    Found existing installation: fastapi 0.105.0\r\n",
      "    Uninstalling fastapi-0.105.0:\r\n",
      "      Successfully uninstalled fastapi-0.105.0\r\n",
      "Successfully installed boto3-1.34.5 botocore-1.34.5 cloudpickle-2.2.1 contextlib2-21.6.0 dill-0.3.7 docker-7.0.0 fastapi-0.95.2 google-pasta-0.2.0 importlib-metadata-6.11.0 jmespath-1.0.1 multiprocess-0.70.15 pandas-2.1.4 pathos-0.3.1 pox-0.3.3 ppft-1.7.6.7 pydantic-1.10.13 pytz-2023.3.post1 s3transfer-0.9.0 sagemaker-2.201.0 schema-0.7.5 smdebug-rulesconfig-1.0.1 tblib-1.7.0 tzdata-2023.3 uvicorn-0.22.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.2.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sagemaker"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T18:30:00.179187Z",
     "start_time": "2023-12-21T18:29:35.419310Z"
    }
   },
   "id": "32923f504937589f"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import io\n",
    "class LineIterator:\n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord('\\n'):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if 'PayloadPart' not in chunk:\n",
    "                print('Unknown event type:' + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk['PayloadPart']['Bytes'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T04:42:00.994454Z",
     "start_time": "2023-12-22T04:42:00.986908Z"
    }
   },
   "id": "a0aaeabd9cae6188"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# stream https://aws.amazon.com/blogs/machine-learning/elevating-the-generative-ai-experience-introducing-streaming-support-in-amazon-sagemaker-hosting/\n",
    "# run !gimme-aws-creds, then aws_switch sofi-bedrock-bdp-production\n",
    "import boto3\n",
    "import io\n",
    "import json \n",
    "\n",
    "# session = boto3.Session(profile_name='AssumeBedrockPOC')\n",
    "session = boto3.Session(profile_name='sofi-bedrock-bdp-production')\n",
    "smr = session.client('sagemaker-runtime', 'us-west-2')\n",
    "#AssumeBedrockPOC\n",
    "\n",
    "def sagemaker_prompt(question, on_text_received):\n",
    "    print(f\"sagemaker question: {question}\")\n",
    "    system_message = \"You are a helpful assistant that does not use superfluous pleasantries. Avoiding ending your reply with questions.  If a question does not make sense, call out why it doesn't make sense, and don't attempt to answer. If you don't know the answer to a question, do not make up an answer.\"\n",
    "    prompt=f'''[INST] <<SYS>>\n",
    "        {system_message}\n",
    "        <</SYS>>\n",
    "        {question} [/INST]'''\n",
    "    \n",
    "    # hyperparameters for llm\n",
    "    payload = {\n",
    "      \"inputs\": prompt,\n",
    "      \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.7,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_k\": 10,\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "        \"stop\": [\"<|endoftext|>\"]\n",
    "      },\n",
    "      \"stream\": True  \n",
    "    }\n",
    "    stop_token = \"<|endoftext|>\"\n",
    "    # llm.deserializer=StreamDeserializer()\n",
    "    endpoint_name = \"Llama-2-13B-Chat-fp16-v7-2023-12-21-17-51-07-902\"\n",
    "    resp = smr.invoke_endpoint_with_response_stream(EndpointName=endpoint_name, Body=json.dumps(payload), ContentType='application/json')\n",
    "    event_stream = resp['Body']\n",
    "    start_json = b'{'\n",
    "    for line in LineIterator(event_stream):\n",
    "        if line != b'' and start_json in line:\n",
    "            data = json.loads(line[line.find(start_json):].decode('utf-8'))\n",
    "            if data['token']['text'] != stop_token:\n",
    "                on_text_received(data['token']['text'])\n",
    "                # print(data['token']['text'],end='')\n",
    "\n",
    "def handle_text_received(text):\n",
    "    print(text, end='')\n",
    "\n",
    "sagemaker_prompt(\"What is a camel?\", handle_text_received)                "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T04:44:12.209809Z",
     "start_time": "2023-12-22T04:44:12.047192Z"
    }
   },
   "id": "d383d2cce2c453c9"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: aws_switch\r\n"
     ]
    }
   ],
   "source": [
    "# !gimme-aws-creds\n",
    "# !aws_switch sofi-bedrock-bdp-production"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T18:52:34.880598Z",
     "start_time": "2023-12-21T18:52:34.635005Z"
    }
   },
   "id": "9e36c33809e5585a"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event: {'type': 'message', 'subtype': 'channel_join', 'channel': 'C06BXU1T6P2', 'text': '<@U02EJ467WLX> has joined the channel', 'user': 'U02EJ467WLX', 'team': 'TB14JBH0C', 'event_ts': '1703197636.366049', 'ts': '1703197636.366049'}event: {'type': 'message', 'channel': 'C06BXU1T6P2', 'text': '<@U06B7U1478S> what is a nucleous?', 'blocks': [{'type': 'rich_text', 'block_id': 'JHt8m', 'elements': [{'type': 'rich_text_section', 'elements': [{'type': 'user', 'user_id': 'U06B7U1478S'}, {'type': 'text', 'text': ' what is a nucleous?'}]}]}], 'user': 'U02EJ467WLX', 'client_msg_id': '20a19d97-6fe7-4c35-ad14-12c4699f0729', 'team': 'TB14JBH0C', 'source_team': 'E01BQQJKZQS', 'user_team': 'E01BQQJKZQS', 'suppress_notification': False, 'event_ts': '1703197661.088719', 'ts': '1703197661.088719'}\n",
      "event: {'type': 'message', 'subtype': 'channel_join', 'channel': 'C06BXU1T6P2', 'text': '<@U06B7U1478S> has joined the channel', 'user': 'U06B7U1478S', 'team': 'TB14JBH0C', 'inviter': 'U02EJ467WLX', 'event_ts': '1703197663.626139', 'ts': '1703197663.626139'}\n",
      "event: {'type': 'message', 'subtype': 'message_replied', 'message': {'client_msg_id': '20a19d97-6fe7-4c35-ad14-12c4699f0729', 'type': 'message', 'text': '<@U06B7U1478S> what is a nucleous?', 'user': 'U02EJ467WLX', 'ts': '1703197661.088719', 'blocks': [{'type': 'rich_text', 'block_id': 'JHt8m', 'elements': [{'type': 'rich_text_section', 'elements': [{'type': 'user', 'user_id': 'U06B7U1478S'}, {'type': 'text', 'text': ' what is a nucleous?'}]}]}], 'team': 'TB14JBH0C', 'thread_ts': '1703197661.088719', 'reply_count': 1, 'reply_users_count': 1, 'latest_reply': '1703197665.079839', 'reply_users': ['B06BLH6F9S5'], 'is_locked': False}, 'channel': 'C06BXU1T6P2', 'hidden': True, 'ts': '1703197661.088719', 'event_ts': '1703197665.000300'}\n",
      "sagemaker question: <@U06B7U1478S> what is a nucleous?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 54\u001B[0m\n\u001B[1;32m     51\u001B[0m         sagemaker_prompt(message_received, handle_text_received)\n\u001B[1;32m     53\u001B[0m \u001B[38;5;66;03m# Running the event loop\u001B[39;00m\n\u001B[0;32m---> 54\u001B[0m asyncio\u001B[38;5;241m.\u001B[39mensure_future(\u001B[43mrtm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     55\u001B[0m asyncio\u001B[38;5;241m.\u001B[39mget_event_loop()\u001B[38;5;241m.\u001B[39mrun_forever()\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/slack_sdk/rtm_v2/__init__.py:270\u001B[0m, in \u001B[0;36mRTMClient.start\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    268\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Establishes an RTM connection and blocks the current thread.\"\"\"\u001B[39;00m\n\u001B[1;32m    269\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconnect()\n\u001B[0;32m--> 270\u001B[0m \u001B[43mEvent\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:581\u001B[0m, in \u001B[0;36mEvent.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    579\u001B[0m signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flag\n\u001B[1;32m    580\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[0;32m--> 581\u001B[0m     signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    582\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:312\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[1;32m    311\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 312\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    313\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    314\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T22:29:07.080735Z",
     "start_time": "2023-12-21T22:25:54.574981Z"
    }
   },
   "id": "a3f8e5cf0a4509c0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Begin new attempt using threading "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78cc7be105de1744"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai_bot_user\n",
      " why is the moon silver?\n",
      "sagemaker question: <@U06B7U1478S> why is the moon silver?\n",
      "ai_bot_user\n",
      " how many shark attacks are there per year on average?\n",
      "\n",
      "sagemaker question: <@U06B7U1478S> how many shark attacks are there per year on average?\n",
      "ai_bot_user\n",
      " write some python code to find prime numbers\n",
      "sagemaker question: <@U06B7U1478S> write some python code to find prime numbers\n",
      "ai_bot_user\n",
      " tell me a story about a Buddhist monk\n",
      "sagemaker question: <@U06B7U1478S> tell me a story about a Buddhist monk\n",
      "ai_bot_user\n",
      " how does fire work?\n",
      "\n",
      "sagemaker question: <@U06B7U1478S> how does fire work?\n",
      "ai_bot_user\n",
      " how do I obtain enlightenment?\n",
      "\n",
      "sagemaker question: <@U06B7U1478S> how do I obtain enlightenment?\n",
      "ai_bot_user\n",
      " what is the likelihood of life beyond death?\n",
      "\n",
      "sagemaker question: <@U06B7U1478S> what is the likelihood of life beyond death?\n",
      "ai_bot_user\n",
      " what are some states of consciousness?\n",
      "\n",
      "sagemaker question: <@U06B7U1478S> what are some states of consciousness?\n",
      "ai_bot_user\n",
      " how do I SSH to a machine?\n",
      "\n",
      "sagemaker question: <@U06B7U1478S> how do I SSH to a machine?\n",
      "ai_bot_user\n",
      " what is 7 / 3?\n",
      "\n",
      "sagemaker question: <@U06B7U1478S> what is 7 / 3?\n",
      "ai_bot_user\n",
      " tell me a riddle\n",
      "\n",
      "sagemaker question: <@U06B7U1478S> tell me a riddle\n",
      "ai_bot_user\n",
      " what are some design patterns in software development?\n",
      "sagemaker question: <@U06B7U1478S> what are some design patterns in software development?\n",
      "Interrupted by user, shutting down.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import threading\n",
    "nest_asyncio.apply()\n",
    "from slack_sdk.rtm_v2 import RTMClient\n",
    "\n",
    "# Load your Slack API token\n",
    "with open('slack-api-token.txt', 'r') as file:\n",
    "    slack_token = file.read().strip()\n",
    "\n",
    "# Initialize the RTMClient\n",
    "rtm = RTMClient(token=slack_token)\n",
    "\n",
    "# Define the event handler\n",
    "@rtm.on(\"message\")\n",
    "def handle(client: RTMClient, event: dict):\n",
    "    # print(f\"event: {event}\")\n",
    "    message_received = event.get('text', '')\n",
    "    # channel_id = event['channel']\n",
    "    # thread_ts = event['ts']\n",
    "    # user = event['user']  # User ID\n",
    "    ai_bot_user = \"<@U06B7U1478S>\"\n",
    "    \n",
    "    # Post a message to the channel\n",
    "    # Check if the message is addressed to @ai\n",
    "    if message_received.startswith(ai_bot_user):\n",
    "        print('ai_bot_user')\n",
    "        \n",
    "        def handle_ai_bot_message(client: RTMClient, event: dict):\n",
    "            # print(f'handle ai bot message: {event}')\n",
    "            message_received = event.get('text', '')\n",
    "            channel_id = event['channel']\n",
    "            thread_ts = event['ts']\n",
    "            \n",
    "            prompt = message_received.replace(ai_bot_user, '')\n",
    "            cumulative_text = f\"{prompt}\\n\"\n",
    "            print(cumulative_text)\n",
    "            # Initial message sent to the channel\n",
    "            initial_response = client.web_client.chat_postMessage(\n",
    "                channel=channel_id,\n",
    "                text=cumulative_text,\n",
    "                thread_ts=thread_ts\n",
    "            )\n",
    "    \n",
    "            # Retrieve timestamp of the initial response\n",
    "            initial_response_ts = initial_response['ts']\n",
    "            last_update_length = len(cumulative_text)\n",
    "            \n",
    "            def handle_text_received(text):\n",
    "                nonlocal cumulative_text, last_update_length\n",
    "                cumulative_text += text\n",
    "                # print(text, end='')\n",
    "                # Edit the existing message with the new text\n",
    "                if len(cumulative_text) - last_update_length >= 20 or \"</s>\" in cumulative_text:\n",
    "                    client.web_client.chat_update(\n",
    "                        channel=channel_id,\n",
    "                        ts=initial_response_ts,  # Timestamp of the message to update\n",
    "                        text=cumulative_text,\n",
    "                        thread_ts=thread_ts\n",
    "                    )\n",
    "                    last_update_length = len(cumulative_text)\n",
    "    \n",
    "            sagemaker_prompt(message_received, handle_text_received)\n",
    "\n",
    "        thread = threading.Thread(\n",
    "            target=handle_ai_bot_message,\n",
    "            args=(client, event)\n",
    "        )\n",
    "        thread.start()\n",
    "        # handle_ai_bot_message(client, event)\n",
    "    \n",
    "# Running the event loop\n",
    "# asyncio.ensure_future(rtm.start())\n",
    "# asyncio.get_event_loop().run_forever()\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "try:\n",
    "    asyncio.ensure_future(rtm.start())\n",
    "    loop.run_forever()\n",
    "except KeyboardInterrupt:\n",
    "    # Handle the interrupt gracefully\n",
    "    print(\"Interrupted by user, shutting down.\")\n",
    "finally:\n",
    "    # Perform any cleanup here if necessary\n",
    "    loop.stop()\n",
    "    # loop.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T13:57:03.880124Z",
     "start_time": "2023-12-22T04:44:16.189502Z"
    }
   },
   "id": "99b1aacf266c22a1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # sagemaker rtm\n",
    "# import asyncio\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "# from slack_sdk.rtm_v2 import RTMClient\n",
    "# \n",
    "# # Load your Slack API token\n",
    "# with open('slack-api-token.txt', 'r') as file:\n",
    "#     slack_token = file.read().strip()\n",
    "# \n",
    "# # Initialize the RTMClient\n",
    "# rtm = RTMClient(token=slack_token)\n",
    "# \n",
    "# # Define the event handler\n",
    "# @rtm.on(\"message\")\n",
    "# def handle(client: RTMClient, event: dict):\n",
    "#     print(f\"event: {event}\")\n",
    "#     message_received = event.get('text', '')\n",
    "#     channel_id = event['channel']\n",
    "#     thread_ts = event['ts']\n",
    "#     user = event['user']  # User ID\n",
    "#     ai_bot_user = \"<@U06B7U1478S>\"\n",
    "#     \n",
    "#     # Post a message to the channel\n",
    "#     # Check if the message is addressed to @ai\n",
    "#     if ai_bot_user in message_received:\n",
    "#         prompt = message_received.replace(ai_bot_user, '')\n",
    "#         cumulative_text = f\"Sending prompt to LLM: {prompt}\"\n",
    "#         # Initial message sent to the channel\n",
    "#         initial_response = client.web_client.chat_postMessage(\n",
    "#             channel=channel_id,\n",
    "#             text=cumulative_text,\n",
    "#             thread_ts=thread_ts\n",
    "#         )\n",
    "# \n",
    "#         # Retrieve timestamp of the initial response\n",
    "#         initial_response_ts = initial_response['ts']\n",
    "# \n",
    "#         def handle_text_received(text):\n",
    "#             global cumulative_text\n",
    "#             cumulative_text += text\n",
    "#             print(text, end='')\n",
    "#             # Edit the existing message with the new text\n",
    "#             client.web_client.chat_update(\n",
    "#                 channel=channel_id,\n",
    "#                 ts=initial_response_ts,  # Timestamp of the message to update\n",
    "#                 text=cumulative_text,\n",
    "#                 thread_ts=thread_ts\n",
    "#             )\n",
    "# \n",
    "#         sagemaker_prompt(message_received, handle_text_received)\n",
    "#     \n",
    "# # Running the event loop\n",
    "# asyncio.ensure_future(rtm.start())\n",
    "# asyncio.get_event_loop().run_forever()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d63d95cd9b2f168"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'groups'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m match \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39mmatch(pattern, message)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(match)\n\u001B[0;32m----> 7\u001B[0m slash_command, url, new_message_with_command_stripped \u001B[38;5;241m=\u001B[39m \u001B[43mmatch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m()\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(url)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'groups'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "message = \"/summarize-url https://api.slack.com/legacy/message-menus\"\n",
    "pattern = r'^(\\/summarize-url)\\s+(https?:\\/\\/[^\\s]+)(.*)$'\n",
    "match = re.match(pattern, message)\n",
    "print(match)\n",
    "\n",
    "slash_command, url, new_message_with_command_stripped = match.groups()\n",
    "print(url)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T20:03:42.310284Z",
     "start_time": "2023-12-27T20:03:42.296507Z"
    }
   },
   "id": "e65c11cd97b77174"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cddfeb11080e5bfc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
