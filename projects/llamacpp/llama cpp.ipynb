{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "https://llama-cpp-python.readthedocs.io/en/latest/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /Users/jmcaffee/llm_models/Llama-3-8B-Instruct-32k-v0.1.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models--MaziyarPanahi--Llama-3-8B-Ins...\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 32000\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 8000000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 32000\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 8000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32000\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = models--MaziyarPanahi--Llama-3-8B-Instruct-32k-v0.1\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4685.30 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 32000\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 8000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  4000.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 4000.00 MiB, K (f16): 2000.00 MiB, V (f16): 2000.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =  2094.51 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128009', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.model': 'gpt2', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32000', 'llama.attention.head_count': '32', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.rope.dimension_count': '128', 'llama.rope.freq_base': '8000000.000000', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'models--MaziyarPanahi--Llama-3-8B-Instruct-32k-v0.1', 'llama.block_count': '32'}\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "max_tokens = 32000\n",
    "# llama_llm = Llama(model_path=\"/Volumes/4Tera_SSD_2022/llm_models/llama-2-7b-chat.Q5_K_M.gguf\", chat_format=\"llama-2\")\n",
    "model_file_path = \"/Users/jmcaffee/llm_models/Llama-3-8B-Instruct-32k-v0.1.Q4_K_M.gguf\"\n",
    "llm = Llama(model_path=model_file_path, chat_format=\"llama-3\", n_ctx=max_tokens)\n",
    "llm.verbose = False # don't print a bunch of stuff\n",
    "# llm = Llama(model_path=\"/Volumes/4Tera_SSD_2022/llm_models/llama-2-13b-chat.Q5_K_M.gguf\", chat_format=\"llama-2\", n_ctx=2048) # n_ctx=2048\n",
    "# function_llm = Llama(model_path=\"/Volumes/4Tera_SSD_2022/llm_models/functionary-7b-v1.Q5_K.gguf\", chat_format=\"llama-2\") # n_ctx=2048"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-05-02T11:37:52.983437Z",
     "start_time": "2024-05-02T11:37:52.292337Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "def prompt(question):\n",
    "    output = llm(\n",
    "        f\"Q: {question} A:\", # Prompt\n",
    "        # max_tokens=512,\n",
    "        max_tokens=max_tokens,\n",
    "        stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "        echo=False, # Echo the prompt back in the output\n",
    "    )\n",
    "    answer = output['choices'][0]['text']\n",
    "    print(f\"Q: {question} \\nA:{answer}\")\n",
    "    return answer\n",
    "\n",
    "def prompt2(question):\n",
    "    start_time_seconds = time.time()\n",
    "    output = llm.create_chat_completion(\n",
    "        max_tokens=max_tokens,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an assistant who perfectly responds to questions with as many details as possible, without making up facts.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    elapsed_time_seconds = time.time() - start_time_seconds\n",
    "    # print(output)\n",
    "    answer = output['choices'][0]['message']['content']\n",
    "    print(f'answer generated in {elapsed_time_seconds} seconds')\n",
    "    print(f\"Q: {question} \\nA:{answer}\")\n",
    "    return answer\n",
    "\n",
    "def stream_prompt(question):\n",
    "    start_time_seconds = time.time()\n",
    "    stream = llm.create_chat_completion(\n",
    "        max_tokens=max_tokens,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an assistant who perfectly responds to questions with as many details as possible, without making up facts.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            }\n",
    "        ],\n",
    "        stream = True,\n",
    "    )\n",
    "\n",
    "\n",
    "    for output in stream:\n",
    "        choices = output['choices']\n",
    "        choice = choices[0]\n",
    "        delta = choice['delta']\n",
    "        if 'content' in delta:\n",
    "            print(delta['content'], end='')\n",
    "    print()\n",
    "\n",
    "    elapsed_time_seconds = time.time() - start_time_seconds\n",
    "    print(f'answer generated in {elapsed_time_seconds} seconds')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-05-02T11:37:59.451963Z",
     "start_time": "2024-05-02T11:37:59.441835Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Juan Sebastián Elcano (1476-1526) was a Spanish explorer and navigator who is best known for being the first person to circumnavigate the globe in a single expedition. He was born in Getaria, Spain, and his full name was Juan Sebastián Elcano y Hurtado de M"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mstream_prompt\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mWho was Juan de Leon?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[8], line 56\u001B[0m, in \u001B[0;36mstream_prompt\u001B[0;34m(question)\u001B[0m\n\u001B[1;32m     39\u001B[0m start_time_seconds \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m     40\u001B[0m stream \u001B[38;5;241m=\u001B[39m llm\u001B[38;5;241m.\u001B[39mcreate_chat_completion(\n\u001B[1;32m     41\u001B[0m     max_tokens\u001B[38;5;241m=\u001B[39mmax_tokens,\n\u001B[1;32m     42\u001B[0m     messages \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     52\u001B[0m     stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     53\u001B[0m )\n\u001B[0;32m---> 56\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m stream:\n\u001B[1;32m     57\u001B[0m     choices \u001B[38;5;241m=\u001B[39m output[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchoices\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     58\u001B[0m     choice \u001B[38;5;241m=\u001B[39m choices[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/llama_cpp/llama_chat_format.py:267\u001B[0m, in \u001B[0;36m_convert_text_completion_chunks_to_chat\u001B[0;34m(chunks)\u001B[0m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_convert_text_completion_chunks_to_chat\u001B[39m(\n\u001B[1;32m    265\u001B[0m     chunks: Iterator[llama_types\u001B[38;5;241m.\u001B[39mCreateCompletionStreamResponse],\n\u001B[1;32m    266\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[llama_types\u001B[38;5;241m.\u001B[39mChatCompletionChunk]:\n\u001B[0;32m--> 267\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(chunks):\n\u001B[1;32m    268\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    269\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m {\n\u001B[1;32m    270\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchat\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m chunk[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    271\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: chunk[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    283\u001B[0m                 ],\n\u001B[1;32m    284\u001B[0m             }\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/llama_cpp/llama.py:1046\u001B[0m, in \u001B[0;36mLlama._create_completion\u001B[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001B[0m\n\u001B[1;32m   1044\u001B[0m finish_reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlength\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1045\u001B[0m multibyte_fix \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1046\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(\n\u001B[1;32m   1047\u001B[0m     prompt_tokens,\n\u001B[1;32m   1048\u001B[0m     top_k\u001B[38;5;241m=\u001B[39mtop_k,\n\u001B[1;32m   1049\u001B[0m     top_p\u001B[38;5;241m=\u001B[39mtop_p,\n\u001B[1;32m   1050\u001B[0m     min_p\u001B[38;5;241m=\u001B[39mmin_p,\n\u001B[1;32m   1051\u001B[0m     typical_p\u001B[38;5;241m=\u001B[39mtypical_p,\n\u001B[1;32m   1052\u001B[0m     temp\u001B[38;5;241m=\u001B[39mtemperature,\n\u001B[1;32m   1053\u001B[0m     tfs_z\u001B[38;5;241m=\u001B[39mtfs_z,\n\u001B[1;32m   1054\u001B[0m     mirostat_mode\u001B[38;5;241m=\u001B[39mmirostat_mode,\n\u001B[1;32m   1055\u001B[0m     mirostat_tau\u001B[38;5;241m=\u001B[39mmirostat_tau,\n\u001B[1;32m   1056\u001B[0m     mirostat_eta\u001B[38;5;241m=\u001B[39mmirostat_eta,\n\u001B[1;32m   1057\u001B[0m     frequency_penalty\u001B[38;5;241m=\u001B[39mfrequency_penalty,\n\u001B[1;32m   1058\u001B[0m     presence_penalty\u001B[38;5;241m=\u001B[39mpresence_penalty,\n\u001B[1;32m   1059\u001B[0m     repeat_penalty\u001B[38;5;241m=\u001B[39mrepeat_penalty,\n\u001B[1;32m   1060\u001B[0m     stopping_criteria\u001B[38;5;241m=\u001B[39mstopping_criteria,\n\u001B[1;32m   1061\u001B[0m     logits_processor\u001B[38;5;241m=\u001B[39mlogits_processor,\n\u001B[1;32m   1062\u001B[0m     grammar\u001B[38;5;241m=\u001B[39mgrammar,\n\u001B[1;32m   1063\u001B[0m ):\n\u001B[1;32m   1064\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1065\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m llama_cpp\u001B[38;5;241m.\u001B[39mllama_token_is_eog(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model\u001B[38;5;241m.\u001B[39mmodel, token):\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/llama_cpp/llama.py:709\u001B[0m, in \u001B[0;36mLlama.generate\u001B[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001B[0m\n\u001B[1;32m    707\u001B[0m \u001B[38;5;66;03m# Eval and sample\u001B[39;00m\n\u001B[1;32m    708\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 709\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    710\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m sample_idx \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_tokens:\n\u001B[1;32m    711\u001B[0m         token \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msample(\n\u001B[1;32m    712\u001B[0m             top_k\u001B[38;5;241m=\u001B[39mtop_k,\n\u001B[1;32m    713\u001B[0m             top_p\u001B[38;5;241m=\u001B[39mtop_p,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    727\u001B[0m             idx\u001B[38;5;241m=\u001B[39msample_idx,\n\u001B[1;32m    728\u001B[0m         )\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/llama_cpp/llama.py:547\u001B[0m, in \u001B[0;36mLlama.eval\u001B[0;34m(self, tokens)\u001B[0m\n\u001B[1;32m    543\u001B[0m n_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch)\n\u001B[1;32m    544\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch\u001B[38;5;241m.\u001B[39mset_batch(\n\u001B[1;32m    545\u001B[0m     batch\u001B[38;5;241m=\u001B[39mbatch, n_past\u001B[38;5;241m=\u001B[39mn_past, logits_all\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext_params\u001B[38;5;241m.\u001B[39mlogits_all\n\u001B[1;32m    546\u001B[0m )\n\u001B[0;32m--> 547\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    548\u001B[0m \u001B[38;5;66;03m# Save tokens\u001B[39;00m\n\u001B[1;32m    549\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_ids[n_past : n_past \u001B[38;5;241m+\u001B[39m n_tokens] \u001B[38;5;241m=\u001B[39m batch\n",
      "File \u001B[0;32m~/git/python-playground-v2/venv/lib/python3.9/site-packages/llama_cpp/_internals.py:315\u001B[0m, in \u001B[0;36m_LlamaContext.decode\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m batch\u001B[38;5;241m.\u001B[39mbatch \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 315\u001B[0m return_code \u001B[38;5;241m=\u001B[39m \u001B[43mllama_cpp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllama_decode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    317\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    319\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_code \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    320\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllama_decode returned \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mreturn_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "stream_prompt(\"Who was Juan de Leon?\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-05-02T11:38:06.725387Z",
     "start_time": "2024-05-02T11:38:01.502007Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the documentation, you can get the account balance by calling the `/accounts/balance/get` endpoint. This endpoint returns the current and available balances for an account, as well as other information about the account's type, subtype, and verification status.\n",
      "answer generated in 62.31879496574402 seconds\n"
     ]
    }
   ],
   "source": [
    "with open('../parse_openapi/plaid-openai-parse-results/plaid_accounts_get.html', 'r') as file:\n",
    "    large_text = file.read()\n",
    "    \n",
    "stream_prompt(f\"\"\"\n",
    "Using the documentation below, what endpoint do I call to get the account balance?\n",
    "{large_text}\n",
    "\"\"\")    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T11:48:53.111185Z",
     "start_time": "2024-05-02T11:47:50.784336Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
